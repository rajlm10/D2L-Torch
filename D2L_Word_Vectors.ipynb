{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D2L_Word_Vectors.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMVvBmpzch8YAUc2ps+DxK8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajlm10/D2L-Torch/blob/main/D2L_Word_Vectors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "woy-bbQevUR8",
        "outputId": "7d3c7fa7-76b2-4cd0-f970-28c8f3565a15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 82 kB 554 kB/s \n",
            "\u001b[K     |████████████████████████████████| 11.2 MB 9.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 15.7 MB 27.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 61 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 22.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 930 kB 58.7 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.25.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install d2l -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "RYZW8arSwSa8"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.DATA_HUB['glove.6b.50d'] = (d2l.DATA_URL + 'glove.6B.50d.zip', '0b8703943ccdb6eb788e6f091b8946e82231bc4d')\n",
        "\n",
        "d2l.DATA_HUB['glove.6b.100d'] = (d2l.DATA_URL + 'glove.6B.100d.zip', 'cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a')\n",
        "\n",
        "d2l.DATA_HUB['glove.42b.300d'] = (d2l.DATA_URL + 'glove.42B.300d.zip', 'b5116e234e9eb9076672cfeabf5469f3eec904fa')\n",
        "\n",
        "d2l.DATA_HUB['wiki.en'] = (d2l.DATA_URL + 'wiki.en.zip', 'c1816da3821ae9f43899be655002f6c723e91b88')"
      ],
      "metadata": {
        "id": "PXvWZsJewVPE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding:\n",
        "  \"\"\"Token Embedding.\"\"\"\n",
        "  def __init__(self, embedding_name):\n",
        "    self.idx_to_token, self.idx_to_vec = self._load_embedding(embedding_name)\n",
        "    self.unknown_idx = 0\n",
        "    self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n",
        "\n",
        "  def _load_embedding(self, embedding_name):\n",
        "    idx_to_token, idx_to_vec = ['<unk>'], []\n",
        "    data_dir = d2l.download_extract(embedding_name)\n",
        "    # GloVe website: https://nlp.stanford.edu/projects/glove/ \n",
        "    # fastText website: https://fasttext.cc/\n",
        "    with open(os.path.join(data_dir, 'vec.txt'), 'r') as f:\n",
        "      for line in f:\n",
        "        elems = line.rstrip().split(' ')\n",
        "        token, elems = elems[0], [float(elem) for elem in elems[1:]] # Skip header information, such as the top row in fastText \n",
        "        if len(elems) > 1:\n",
        "          idx_to_token.append(token)\n",
        "          idx_to_vec.append(elems)\n",
        "    idx_to_vec = [[0] * len(idx_to_vec[0])] + idx_to_vec #for the unk token, all 0s\n",
        "    return idx_to_token, torch.tensor(idx_to_vec)\n",
        "\n",
        "  def __getitem__(self, tokens):\n",
        "    indices = [self.token_to_idx.get(token, self.unknown_idx) for token in tokens]\n",
        "    vecs = self.idx_to_vec[torch.tensor(indices)] \n",
        "    return vecs\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)"
      ],
      "metadata": {
        "id": "-fAtz1uCwmGk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove_6b50d = TokenEmbedding('glove.6b.50d')\n",
        "len(glove_6b50d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2fm1xSox5Ix",
        "outputId": "6b148b54-957f-4f06-a70f-24fb69364f03"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "400001"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_6b50d.token_to_idx['beautiful'], glove_6b50d.idx_to_token[3367]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNvdYsuEx9OM",
        "outputId": "400ead9f-80c9-428a-f38b-18cc54d09b66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3367, 'beautiful')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_6b50d[['pretty']]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXRIS3gTzIM-",
        "outputId": "9ae6f6be-a5f7-47fb-de16-fa484623d462"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.2492, -0.3984, -0.4585, -0.3485,  0.7494, -0.3157, -0.4866,  0.0888,\n",
              "         -0.5066,  0.4843, -0.6611,  0.0834, -0.6331,  0.3835,  0.5399,  0.1442,\n",
              "          0.5899,  0.2353, -0.0325, -0.9441, -0.9784,  0.7925,  0.3346,  0.0793,\n",
              "          1.0367, -1.1998, -1.1811,  1.3858,  1.4019, -0.5025,  2.9963, -0.0218,\n",
              "          0.7850,  0.0100,  0.1198, -0.0169,  0.0850,  0.7879, -0.1398, -1.1586,\n",
              "         -0.4945, -0.0492, -0.0585,  0.4244,  0.2616, -0.0854,  0.1407, -0.1651,\n",
              "          0.4529,  1.3669]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_6b50d.idx_to_vec.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhUIOU4zzmzR",
        "outputId": "00ceb460-6910-442b-ed77-d160e9c22f4c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([400001, 50])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Word similarity"
      ],
      "metadata": {
        "id": "uenbjvxeyi-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def knn(W, x, k):\n",
        "  # Add 1e-9 for numerical stability \n",
        "  cos = torch.mv(W, x.reshape(-1,)) / (torch.sqrt(torch.sum(W * W, axis=1) + 1e-9) * torch.sqrt((x * x).sum()))\n",
        "  vals, topk = torch.topk(cos, k=k)\n",
        "  return topk, vals"
      ],
      "metadata": {
        "id": "ugZmu2JPykWJ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_similar_tokens(query_token, k, embed):\n",
        "  topk, cos = knn(embed.idx_to_vec, embed[[query_token]], k + 1) \n",
        "  for i, c in zip(topk[1:], cos[1:]): # Exclude the input word\n",
        "    print(f'cosine sim={float(c):.3f}: {embed.idx_to_token[int(i)]}')"
      ],
      "metadata": {
        "id": "NM7T-_x2zdbS"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_similar_tokens('messi', 3, glove_6b50d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG6zLnW3ztGg",
        "outputId": "cd998b1d-d128-493f-8c10-bcdda9c55bc9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cosine sim=0.937: ronaldinho\n",
            "cosine sim=0.902: rivaldo\n",
            "cosine sim=0.899: ronaldo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word Analogy\n",
        "\n",
        "Besides finding similar words, we can also apply word vectors to word analogy tasks. For example, “man”:“woman”::“son”:“daughter” is the form of a word analogy: “man” is to “woman” as “son” is to “daughter”. Specifically, the word analogy completion task can be defined as: for a word analogy a : b :: c : d, given the first three words a, b and c, find d. Denote the vector of word w by vec(w). To complete the analogy, we will find the word whose vector is most similar to the result of vec(c) + vec(b) − vec(a)."
      ],
      "metadata": {
        "id": "3_cS3ta1z0Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_analogy(token_a, token_b, token_c, embed): \n",
        "  vecs = embed[[token_a, token_b, token_c]]\n",
        "  x = vecs[2]+ (vecs[1] - vecs[0]) \n",
        "  topk, cos = knn(embed.idx_to_vec, x, 1)\n",
        "  return embed.idx_to_token[int(topk[0])] # Remove unknown words"
      ],
      "metadata": {
        "id": "fJRoxMaz0iOe"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_analogy('china', 'beijing', 'india', glove_6b50d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rkXEgSy100r7",
        "outputId": "10a09ae7-52a7-4d03-b840-d20e03dd3b97"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'delhi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    }
  ]
}