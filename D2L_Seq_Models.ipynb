{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "D2L_Seq_Models.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNzQGMhCRrMdeERj5Oii15+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajlm10/D2L-Torch/blob/main/D2L_Seq_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-UytPVNO74rC"
      },
      "outputs": [],
      "source": [
        "!pip install d2l\n",
        "#Restart runtime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils "
      ],
      "metadata": {
        "id": "QgcP5OCd7_3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import re\n",
        "from d2l import torch as d2l\n",
        "import torch\n",
        "import math\n",
        "import random\n",
        "from torch import nn\n",
        "from torch.nn import functional as F \n"
      ],
      "metadata": {
        "id": "4jOaiehj79WQ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d2l.DATA_HUB['time_machine'] = (d2l.DATA_URL + 'timemachine.txt', '090b5e7e70c295757f55df93cb0a180b9691891a')\n",
        "\n",
        "def read_time_machine():\n",
        "  \"\"\"Load the time machine dataset into a list of text lines.\"\"\" \n",
        "  with open(d2l.download('time_machine'), 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  return [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]\n",
        "\n",
        "lines = read_time_machine() \n",
        "print(f'# text lines: {len(lines)}') \n",
        "print(lines[0])\n",
        "print(lines[10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mnRy3aD8Cym",
        "outputId": "b4423bdd-1c94-417f-cf80-4d31454295af"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# text lines: 3221\n",
            "the time machine by h g wells\n",
            "twinkled and his usually pale face was flushed and animated the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization\n",
        "def tokenize(lines,token='word'):\n",
        "  if token=='word':\n",
        "    return [line.split() for line in lines]\n",
        "  if token=='char':\n",
        "    return [list(line) for line in lines]\n",
        "  else:\n",
        "    print('ERROR: unknown token type: ' + token)"
      ],
      "metadata": {
        "id": "InxcMOkH8FKr"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocab:\n",
        "  def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n",
        "    if tokens is None:\n",
        "      tokens = []\n",
        "    if reserved_tokens is None: \n",
        "      reserved_tokens = []\n",
        "    # Sort according to frequencies\n",
        "    counter = count_corpus(tokens)\n",
        "    self._token_freqs = sorted(counter.items(), key=lambda x: x[1],reverse=True)\n",
        "\n",
        "    #Build vocab on init\n",
        "    self.idx_to_token=['<unk>']+reserved_tokens #List\n",
        "    self.token_to_idx={token:idx for idx,token in enumerate(self.idx_to_token)} #Dict\n",
        "\n",
        "    for token,freq in self._token_freqs:\n",
        "      #Don't include tokens with freq<min_freq in the vocab\n",
        "      if freq<min_freq:\n",
        "        break\n",
        "      if token not in self.idx_to_token:\n",
        "        self.idx_to_token.append(token)\n",
        "        self.token_to_idx[token]=len(self.idx_to_token)-1\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.idx_to_token)\n",
        "\n",
        "  @property\n",
        "  def unk(self):\n",
        "    # Index for the unknown token\n",
        "    return 0\n",
        "  \n",
        "  @property\n",
        "  def token_freqs(self):\n",
        "    # Counter object \n",
        "    return self._token_freqs\n",
        "  \n",
        "  def __getitem__(self,tokens):\n",
        "    if not isinstance(tokens, (list, tuple)):\n",
        "      return self.token_to_idx.get(tokens, self.unk) \n",
        "    #If tokens is a list \n",
        "    return [self.__getitem__(token) for token in tokens]\n",
        "\n",
        "  def to_tokens(self, indices):\n",
        "    if not isinstance(indices, (list, tuple)):\n",
        "      return self.idx_to_token[indices]\n",
        "    #If indices is a list\n",
        "    return [self.idx_to_token[index] for index in indices]\n",
        "\n",
        "\n",
        "def count_corpus(tokens):\n",
        "  \"\"\"Count token frequencies.\"\"\"\n",
        "  # Here `tokens` is a 1D list or 2D list\n",
        "  if len(tokens) == 0 or isinstance(tokens[0], list):\n",
        "    # Flatten a list of token lists into a 1D list\n",
        "    tokens = [token for line in tokens for token in line]\n",
        "  #If its already a 1D list,return  \n",
        "  return collections.Counter(tokens)\n"
      ],
      "metadata": {
        "id": "c3HtkOeo8Hh1"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_corpus_time_machine(max_tokens=-1,level='char'):\n",
        "  \"\"\"Return token indices and the vocabulary of the time machine dataset.\"\"\" \n",
        "  lines = read_time_machine()\n",
        "  tokens = tokenize(lines, level)\n",
        "  vocab = Vocab(tokens)\n",
        "  # Call getitem on a single token and append it to a list\n",
        "  corpus = [vocab[token] for line in tokens for token in line]\n",
        "  if max_tokens > 0:\n",
        "    corpus = corpus[:max_tokens] #First max_tokens tokens\n",
        "  return corpus, vocab\n"
      ],
      "metadata": {
        "id": "AA35AEJz8LhN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "def seq_data_iter_random(corpus, batch_size, num_steps):\n",
        "  \"\"\"Generate a minibatch of subsequences using random sampling.\"\"\"\n",
        "  # Start with a random offset (inclusive of `num_steps - 1`) to partition a # sequence\n",
        "  corpus = corpus[random.randint(0, num_steps - 1):]\n",
        "  # Subtract 1 since we need to account for labels\n",
        "  num_subseqs = (len(corpus) - 1) // num_steps\n",
        "  # The starting indices for subsequences of length `num_steps` \n",
        "  initial_indices = list(range(0, num_subseqs * num_steps, num_steps))\n",
        "  # In random sampling, the subsequences from two adjacent random\n",
        "  # minibatches during iteration are not necessarily adjacent on the\n",
        "  # original sequence\n",
        "  random.shuffle(initial_indices)\n",
        "\n",
        "  def data(pos):\n",
        "  # Return a sequence of length `num_steps` starting from `pos` \n",
        "    return corpus[pos: pos + num_steps]\n",
        "  \n",
        "  num_batches = num_subseqs // batch_size\n",
        "  for i in range(0, batch_size * num_batches, batch_size):\n",
        "    # Here, `initial_indices` contains randomized starting indices for # subsequences\n",
        "    initial_indices_per_batch = initial_indices[i: i + batch_size]\n",
        "    X = [data(j) for j in initial_indices_per_batch]\n",
        "    Y = [data(j + 1) for j in initial_indices_per_batch] \n",
        "    yield torch.tensor(X), torch.tensor(Y)"
      ],
      "metadata": {
        "id": "GDVb8wX3CNHN"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_data_iter_sequential(corpus, batch_size, num_steps):\n",
        "  \"\"\"Generate a minibatch of subsequences using sequential partitioning.\"\"\" \n",
        "  # Start with a random offset to partition a sequence\n",
        "  offset = random.randint(0, num_steps)\n",
        "  num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size\n",
        "  Xs = torch.tensor(corpus[offset: offset + num_tokens])\n",
        "  Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])\n",
        "  Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1) \n",
        "  num_batches = Xs.shape[1] // num_steps\n",
        "  for i in range(0, num_steps * num_batches, num_steps):\n",
        "    X = Xs[:, i: i + num_steps]\n",
        "    Y = Ys[:, i: i + num_steps] \n",
        "    yield X, Y"
      ],
      "metadata": {
        "id": "oYjCO3_bCNyZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SeqDataLoader:\n",
        "  \"\"\"An iterator to load sequence data.\"\"\"\n",
        "  def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):\n",
        "    if use_random_iter:\n",
        "      self.data_iter_fn = seq_data_iter_random\n",
        "    else:\n",
        "      self.data_iter_fn = seq_data_iter_sequential\n",
        "    \n",
        "    self.corpus, self.vocab = load_corpus_time_machine(max_tokens) \n",
        "    self.batch_size, self.num_steps = batch_size, num_steps\n",
        "\n",
        "  def __iter__(self):\n",
        "    return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)"
      ],
      "metadata": {
        "id": "99-DMl6vCQdW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):\n",
        "  \"\"\"Return the iterator and the vocabulary of the time machine dataset.\"\"\"\n",
        "  data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)\n",
        "  return data_iter, data_iter.vocab"
      ],
      "metadata": {
        "id": "shaVo2-VCSH3"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RNNs "
      ],
      "metadata": {
        "id": "q3XKakb28N9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(prefix, num_preds, net, vocab, device): \n",
        "  \"\"\"Generate new characters following the `prefix`.\"\"\"\n",
        "  state = net.begin_state(batch_size=1, device=device) \n",
        "  outputs = [vocab[prefix[0]]] # gives [token]\n",
        "  get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1)) \n",
        "  for y in prefix[1:]: # Warm-up period\n",
        "    _, state = net(get_input(), state)\n",
        "    outputs.append(vocab[y]) #Append input text i.e is prefix\n",
        "\n",
        "  for _ in range(num_preds): # Predict `num_preds` steps\n",
        "    y, state = net(get_input(), state) #Keep updating state and input\n",
        "    outputs.append(int(y.argmax(dim=1).reshape(1))) \n",
        "  return ''.join([vocab.idx_to_token[i] for i in outputs])"
      ],
      "metadata": {
        "id": "0zxuyVv2CVtE"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def grad_clipping(net, theta):\n",
        "  \"\"\"Clip the gradient.\"\"\"\n",
        "  if isinstance(net, nn.Module):\n",
        "    params = [p for p in net.parameters() if p.requires_grad] \n",
        "  else:\n",
        "    params = net.params\n",
        "  norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params)) \n",
        "  if norm > theta:\n",
        "    for param in params: \n",
        "      param.grad[:] *= theta / norm"
      ],
      "metadata": {
        "id": "2AQsYKBRCXM-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(net, train_iter, loss, optimizer, device, use_random_iter): \n",
        "  \"\"\"Train a net within one epoch\"\"\"\n",
        "  state, timer = None, d2l.Timer()\n",
        "  metric = d2l.Accumulator(2) # Sum of training loss, no. of tokens\n",
        "  for X, Y in train_iter:\n",
        "    if state is None or use_random_iter:\n",
        "      # Initialize `state` when either it is the first iteration or \n",
        "      # using random sampling\n",
        "      state = net.begin_state(batch_size=X.shape[0], device=device)\n",
        "    else:\n",
        "      if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
        "        # `state` is a tensor for `nn.GRU`\n",
        "        state.detach_() \n",
        "      else:\n",
        "        # `state` is a tuple of tensors for `nn.LSTM` and # for our custom scratch implementation\n",
        "        for s in state:\n",
        "          s.detach_() \n",
        "    y = Y.T.reshape(-1)\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_hat, state = net(X, state)\n",
        "    l = loss(y_hat, y.long()).mean()\n",
        "    \n",
        "    if isinstance(optimizer, torch.optim.Optimizer):\n",
        "      optimizer.zero_grad() \n",
        "      l.backward() \n",
        "      grad_clipping(net, 1) \n",
        "      optimizer.step()\n",
        "\n",
        "    metric.add(l * y.numel(), y.numel())\n",
        "  return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
      ],
      "metadata": {
        "id": "IpGYF6t48Pzw"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(net, train_iter, vocab,loss, optimizer, num_epochs, device, use_random_iter=False):\n",
        "  \"\"\"Train a model\"\"\"\n",
        "  for epoch in range(num_epochs):\n",
        "    ppl, speed = train_epoch(net, train_iter, loss, optimizer, device, use_random_iter)\n",
        "    if (epoch + 1) % 50 == 0:\n",
        "      print(f'perplexity {ppl:.3f}, {speed:.1f} tokens/sec on {str(device)}') "
      ],
      "metadata": {
        "id": "C319Ke2xA34M"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size, num_steps = 32, 35\n",
        "train_iter, vocab = load_data_time_machine(batch_size, num_steps)"
      ],
      "metadata": {
        "id": "O0IB_sXe9wUH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "  \"\"\"The RNN model.\"\"\"\n",
        "  def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
        "    super(RNNModel, self).__init__(**kwargs)\n",
        "    self.rnn = rnn_layer #Can be a RNN,GRU or LSTM\n",
        "    self.vocab_size = vocab_size\n",
        "    self.num_hiddens = self.rnn.hidden_size\n",
        "\n",
        "    # If the RNN is bidirectional (to be introduced later), \n",
        "    # `num_directions` should be 2, else it should be 1.\n",
        "    if not self.rnn.bidirectional:\n",
        "      self.num_directions = 1\n",
        "      self.linear = nn.Linear(self.num_hiddens, self.vocab_size)\n",
        "    else:\n",
        "      self.num_directions = 2\n",
        "      self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)\n",
        "\n",
        "  def forward(self, inputs, state):\n",
        "    X = F.one_hot(inputs.T.long(), self.vocab_size) #inputs.T=num_steps X batch_size\n",
        "    X = X.to(torch.float32)\n",
        "    Y, state = self.rnn(X, state)\n",
        "    # The fully connected layer will first change the shape of `Y` to # (`num_steps` * `batch_size`, `num_hiddens`). \n",
        "    #Its output shape after the linear layer is # (`num_steps` * `batch_size`, `vocab_size`).\n",
        "    output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
        "    return output, state\n",
        "\n",
        "  def begin_state(self, device, batch_size=1): \n",
        "    if not isinstance(self.rnn, nn.LSTM):\n",
        "    # `nn.GRU` takes a tensor as hidden state\n",
        "      return torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens),device=device)\n",
        "    \n",
        "    else:\n",
        "    # `nn.LSTM` takes a tuple of hidden states return for C and H\n",
        "      ( torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device),\n",
        "       torch.zeros((self.num_directions * self.rnn.num_layers, batch_size, self.num_hiddens), device=device) )\n"
      ],
      "metadata": {
        "id": "4oavyOCB-Ajz"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = d2l.try_gpu()\n",
        "num_hiddens=256\n",
        "net = RNNModel(nn.RNN(len(vocab), num_hiddens), vocab_size=len(vocab))\n",
        "net = net.to(device)\n",
        "predict('time traveller', 10, net, vocab, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "jDAvSJhkABHc",
        "outputId": "77146158-ffca-42eb-f0bb-f0d686d521c5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time traveller          '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs, lr = 500, 1\n",
        "\n",
        "loss=nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr)\n",
        "train(net, train_iter, vocab,loss,optimizer, num_epochs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzud5PTQAihp",
        "outputId": "be1cf4a7-e8df-4f8b-f4ba-b56d1689a370"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity 7.334, 114887.5 tokens/sec on cuda:0\n",
            "perplexity 3.690, 110155.8 tokens/sec on cuda:0\n",
            "perplexity 2.029, 109621.7 tokens/sec on cuda:0\n",
            "perplexity 1.591, 109958.8 tokens/sec on cuda:0\n",
            "perplexity 1.478, 111110.3 tokens/sec on cuda:0\n",
            "perplexity 1.357, 110368.3 tokens/sec on cuda:0\n",
            "perplexity 1.337, 109984.6 tokens/sec on cuda:0\n",
            "perplexity 1.341, 108889.6 tokens/sec on cuda:0\n",
            "perplexity 1.329, 110086.7 tokens/sec on cuda:0\n",
            "perplexity 1.254, 109893.9 tokens/sec on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict('time traveller', 50, net, vocab, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "GVCu4UbWERew",
        "outputId": "59f8f30a-7a4a-46dd-b9f4-5f0a688e3793"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time traveller froce ted ale predery aid hey timickon time erd a'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRU"
      ],
      "metadata": {
        "id": "GASZ5XPdErXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gru_layer = nn.GRU(len(vocab), num_hiddens)\n",
        "model = RNNModel(gru_layer, len(vocab))\n",
        "model = model.to(device)\n",
        "predict('time traveller', 50, model, vocab, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "koq2FD7OEswF",
        "outputId": "f9b7f145-d292-464d-dc83-2bee6bb0e0a7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time travellerzzlzzlzzzlzzzlzzzlzzzlzzzlzzzlzzzlzzzlzzzlzzzlzzzl'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "train(model, train_iter, vocab,loss,optimizer, num_epochs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRHc7H8wFHll",
        "outputId": "97b5f1bf-37a9-4af8-a222-db54463c410d"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity 9.561, 122190.3 tokens/sec on cuda:0\n",
            "perplexity 7.706, 124487.7 tokens/sec on cuda:0\n",
            "perplexity 5.952, 123327.1 tokens/sec on cuda:0\n",
            "perplexity 3.616, 122662.1 tokens/sec on cuda:0\n",
            "perplexity 1.648, 121400.1 tokens/sec on cuda:0\n",
            "perplexity 1.154, 116743.0 tokens/sec on cuda:0\n",
            "perplexity 1.074, 122402.0 tokens/sec on cuda:0\n",
            "perplexity 1.050, 124860.3 tokens/sec on cuda:0\n",
            "perplexity 1.048, 102389.3 tokens/sec on cuda:0\n",
            "perplexity 1.051, 120393.5 tokens/sec on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict('time traveller', 50, model, vocab, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0YUnKyWHFwY2",
        "outputId": "e28408d6-2618-499f-97e2-fbdba7900c1a"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time travelleryou can show black is white by argument said filby'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM"
      ],
      "metadata": {
        "id": "uwZALi3CFyeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_layer = nn.LSTM(len(vocab), num_hiddens)\n",
        "model = RNNModel(lstm_layer, len(vocab))\n",
        "model = model.to(device)\n",
        "predict('time traveller', 50, model, vocab, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "l1fH8BfTF0Ij",
        "outputId": "cab73ac9-0849-4573-e6e0-fbdec46bfa59"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time travellerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "train(model, train_iter, vocab,loss,optimizer, num_epochs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETfAG5ptGJyL",
        "outputId": "65e7c73c-3d15-42e5-dc44-cc507d392f9d"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity 11.062, 104974.2 tokens/sec on cuda:0\n",
            "perplexity 8.569, 103188.3 tokens/sec on cuda:0\n",
            "perplexity 6.704, 104276.3 tokens/sec on cuda:0\n",
            "perplexity 4.381, 102547.4 tokens/sec on cuda:0\n",
            "perplexity 2.291, 103186.3 tokens/sec on cuda:0\n",
            "perplexity 1.303, 104121.7 tokens/sec on cuda:0\n",
            "perplexity 1.091, 103596.5 tokens/sec on cuda:0\n",
            "perplexity 1.052, 102812.0 tokens/sec on cuda:0\n",
            "perplexity 1.062, 102441.2 tokens/sec on cuda:0\n",
            "perplexity 1.045, 103102.5 tokens/sec on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict('time traveller', 50, model, vocab, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "vgvTohwnGLY7",
        "outputId": "c73e8ecb-1b07-4339-d0ef-4068c847a4e9"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time traveller for so it will be convenient to speak of himwas e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stacked LSTM "
      ],
      "metadata": {
        "id": "8o4lXV-NIrZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_stacks=2\n",
        "lstm_layer = nn.LSTM(len(vocab), num_hiddens,num_stacks)\n",
        "model = RNNModel(lstm_layer, len(vocab))\n",
        "model = model.to(device)\n",
        "predict('time traveller', 50, model, vocab, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "dNgsEN2cItI8",
        "outputId": "4c2fe68c-5fd9-4729-b7c6-7c317450f634"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time travellertttttttttttttttttttttttttttttttttttttttttttttttttt'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss=nn.CrossEntropyLoss()\n",
        "lr=2 #complex architecture\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr)\n",
        "train(model, train_iter, vocab,loss,optimizer, num_epochs, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJXMyTkyI1s2",
        "outputId": "d4dfa762-dc59-4666-d77a-841bb691e84e"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "perplexity 14.677, 80719.8 tokens/sec on cuda:0\n",
            "perplexity 9.309, 80687.4 tokens/sec on cuda:0\n",
            "perplexity 5.698, 81486.1 tokens/sec on cuda:0\n",
            "perplexity 1.854, 81052.3 tokens/sec on cuda:0\n",
            "perplexity 1.107, 81331.2 tokens/sec on cuda:0\n",
            "perplexity 1.048, 81358.7 tokens/sec on cuda:0\n",
            "perplexity 1.033, 80827.4 tokens/sec on cuda:0\n",
            "perplexity 1.033, 80866.8 tokens/sec on cuda:0\n",
            "perplexity 1.031, 82017.4 tokens/sec on cuda:0\n",
            "perplexity 1.021, 80632.2 tokens/sec on cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predict('time traveller', 50, model, vocab, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zWnXaLw6I3gc",
        "outputId": "331d2eb3-0ae2-4cd1-b21a-1c046f4967c9"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time traveller for so it will be convenient to speak of himwas e'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional LSTM"
      ],
      "metadata": {
        "id": "FTLIZKNkI5OM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#To be added"
      ],
      "metadata": {
        "id": "Z5youEa7I8zq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}